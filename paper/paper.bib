@misc{Buchfink2014,
  abstract  = {The alignment of sequencing reads against a protein reference database is a major computational bottleneck in metagenomics and data-intensive evolutionary projects. Although recent tools offer improved performance over the gold standard BLASTX, they exhibit only a modest speedup or low sensitivity. We introduce DIAMOND, an open-source algorithm based on double indexing that is 20,000 times faster than BLASTX on short reads and has a similar degree of sensitivity.},
  author    = {Buchfink, Benjamin and Xie, Chao and Huson, Daniel H.},
  booktitle = {Nature Methods},
  doi       = {10.1038/nmeth.3176},
  issn      = {15487105},
  title     = {{Fast and sensitive protein alignment using DIAMOND}},
  year      = {2014}
}
@article{afgan2018galaxy,
  title     = {The Galaxy platform for accessible, reproducible and collaborative biomedical analyses: 2018 update},
  author    = {Afgan, Enis and Baker, Dannon and Batut, B{\'e}r{\'e}nice and Van Den Beek, Marius and Bouvier, Dave and {\v{C}}ech, Martin and Chilton, John and Clements, Dave and Coraor, Nate and Gr{\"u}ning, Bj{\"o}rn A and others},
  journal   = {Nucleic acids research},
  volume    = {46},
  number    = {W1},
  pages     = {W537--W544},
  year      = {2018},
  publisher = {Oxford University Press},
  doi       = {10.1093/nar/gkaa554}
}
@article{de2012provenance,
  title     = {A provenance-based adaptive scheduling heuristic for parallel scientific workflows in clouds},
  author    = {de Oliveira, Daniel and Oca{\~n}a, Kary ACS and Bai{\~a}o, Fernanda and Mattoso, Marta},
  journal   = {Journal of grid Computing},
  volume    = {10},
  number    = {3},
  pages     = {521--552},
  year      = {2012},
  publisher = {Springer},
  doi       = {10.1007/s10723-012-9227-2}
}
@book{rse-py,
  author    = {Damien Irving and Kate Hertweck and Luke Johnston and Joel Ostblom and Charlotte Wickham and Greg Wilson},
  title     = {Research Software Engineering with Python: Building Software that Makes Research Possible},
  publisher = {CRC Press/Taylor and Francis},
  year      = {2021},
  isbn      = {978-0367698348},
  link      = {https://merely-useful.tech/py-rse/}
}
@article{Costa2013,
  abstract = {Scientific workflows are commonly used to model and execute large-scale scientific experiments. They represent key resources for scientists and are enacted and managed by Scientific Workflow Management Systems (SWfMS). Each SWfMS has its particular approach to execute workflows and to capture and manage their provenance data. Due to the large scale of experiments, it may be unviable to analyze provenance data only after the end of the execution. A single experiment may demand weeks to run, even in high performance computing environments. Thus scientists need to monitor the experiment during its execution, and this can be done through provenance data. Runtime provenance analysis allows for scientists to monitor workflow execution and to take actions before the end of it (i.e. workflow steering). This provenance data can also be used to fine-tune the parallel execution of the workflow dynamically. We use the PROV data model as a basic framework for modeling and providing runtime provenance as a database that can be queried even during the execution. This database is agnostic of SWfMS and workflow engine. We show the benefits of representing and sharing runtime provenance data for improving the experiment management as well as the analysis of the scientific data. {\textcopyright} 2013 ACM.},
  author   = {Costa, Flavio and Silva, V{\'{i}}tor and {De Oliveira}, Daniel and Oca{\~{n}}a, Kary and Ogasawara, Eduardo and Dias, Jonas and Mattoso, Marta},
  doi      = {10.1145/2457317.2457365},
  isbn     = {9781450315999},
  journal  = {ACM International Conference Proceeding Series},
  keywords = {PROV model,import cartridge,scientific workflows},
  number   = {April},
  pages    = {282--289},
  title    = {{Capturing and querying workflow runtime provenance with PROV: A practical approach}},
  year     = {2013}
}
@article{Stevens2007,
  abstract = {This article offers a briefing in one of the knowledge management issues of in silico experimentation in bioinformatics. Recording of the provenance of an experiment-what was done; where, how and why, etc. is an important aspect of scientific best practice that should be extended to in silico experimentation. We will do this in the context of eScience which has been part of the move of bioinformatics towards an industrial setting. Despite the computational nature of bioinformatics, these analyses are scientific and thus necessitate their own versions of typical scientific rigour. Just as recording who, what, why, when, where and how of an experiment is central to the scientific process in laboratory science, so it should be in silico science. The generation and recording of these aspects, or provenance, of an experiment are necessary knowledge management goals if we are to introduce scientific rigour into routine bioinformatics. In Silico experimental protocols should themselves be a form of managing the knowledge of how to perform bioinformatics analyses. Several systems now exist that offer support for the generation and collection of provenance information about how a particular in silico experiment was run, what results were generated, how they were generated, etc. In reviewing provenance support, we will review one of the important knowledge management issues in bioinformatics.},
  author   = {Stevens, Robert and Zhao, Jun and Goble, Carole},
  doi      = {10.1093/bib/bbm015},
  issn     = {14675463},
  journal  = {Briefings in Bioinformatics},
  keywords = {Data derivation,In Silico experiments,Provenance,Validation and verification of results,Workflow},
  number   = {3},
  pages    = {183--194},
  title    = {{Using provenance to manage knowledge of In Silico experiments}},
  volume   = {8},
  year     = {2007}
}
@article{hull2006taverna,
  title     = {Taverna: a tool for building and running workflows of services},
  author    = {Hull, Duncan and Wolstencroft, Katy and Stevens, Robert and Goble, Carole and Pocock, Mathew R and Li, Peter and Oinn, Tom},
  journal   = {Nucleic acids research},
  volume    = {34},
  number    = {suppl\_2},
  pages     = {W729--W732},
  year      = {2006},
  publisher = {Oxford University Press}
}

@article{Buneman2001,
  author  = {Buneman, Peter and Khanna, Sanjeev and Tan, Wang-Chiew and Chiew, Wang -},
  journal = {Lecture Notes in Computer Science},
  number  = {January},
  pages   = {316--330},
  title   = {{Why and Where: A Characterization of Data Provenance}},
  url     = {http://homepages.inf.ed.ac.uk/opb/papers/ICDT2001.pdf},
  volume  = {1973},
  year    = {2001},
  doi     = {10.1007/3-540-44503-x_20}
}
@article{camacho2009blast,
  author    = {Camacho, Christiam and Coulouris, George and Avagyan, Vahram and Ma, Ning and Papadopoulos, Jason and Bealer, Kevin and Madden, Thomas L},
  journal   = {BMC bioinformatics},
  number    = {1},
  pages     = {421},
  publisher = {BioMed Central},
  title     = {{BLAST+: architecture and applications}},
  volume    = {10},
  year      = {2009}
}
@article{Cock2009,
  author  = {Cock, Peter J.A. and Antao, Tiago and Chang, Jeffrey T. and Chapman, Brad A. and Cox, Cymon J. and Dalke, Andrew and Friedberg, Iddo and Hamelryck, Thomas and Kauff, Frank and Wilczynski, Bartek and {De Hoon}, Michiel J.L.},
  doi     = {10.1093/bioinformatics/btp163},
  issn    = {13674803},
  journal = {Bioinformatics},
  title   = {{Biopython: Freely available Python tools for computational molecular biology and bioinformatics}},
  year    = {2009}
}
@article{Ocana2014,
  abstract        = {Computer-aided drug design techniques are important assets in pharmaceutical industry because of their support for research and development of new drugs. Molecular docking (MD) predicts specific compound's binding modes within the active site of target proteins. Since MD is a time-consuming process, existing approaches reduce the number of receptors or ligands in docking by evaluating only small sets of compounds. This restriction in the search space reduces the chances to uniformly cover the diverse space of compounds and misses opportunities to recognize whether new drugs can be identified. Another difficulty with large-scale is analyzing the results, e.g. browsing all directories manually to find which pairs were docked successfully. To address these issues we explored the potential of data provenance analysis and parallel processing of SciCumulus, a cloud Scientific Workflow Management System. We present SciDock, a molecular docking-based virtual screening workflow and evaluate its execution using 10,000 receptor-ligand pairs related to proteases enzymes of protozoan genomes. The overall performance of SciDock using 32 cores, in cloud virtual machines, reaches improvements up to 95.4{\%} when running SciDock with AutoDock and 96.1{\%} when running SciDock with Vina. We show how data provenance improved the result analysis and how it may indicate potential proteases drug targets for protozoan treatments.},
  author          = {Oca{\~{n}}a, Kary and Benza, Silvia and {De Oliveira}, Daniel and Dias, Jonas and Mattoso, Marta},
  doi             = {10.1109/IPDPSW.2014.65},
  isbn            = {9780769552088},
  issn            = {23321237},
  journal         = {Proceedings of the International Parallel and Distributed Processing Symposium, IPDPS},
  keywords        = {Cloud,Drug discovery,Workflow},
  mendeley-groups = {MestradoEngnr/Disserta{\c{c}}{\~{a}}o/Papers Marta,MestradoEngnr/Disserta{\c{c}}{\~{a}}o/Texto principal/Workflows e bioinform{\'{a}}tica},
  pages           = {536--545},
  title           = {{Exploring large scale receptor-Ligand pairs in molecular docking workflows in HPC clouds}},
  year            = {2014}
}
@inproceedings{Ocana2015,
  abstract        = {Homology modeling (HM) plays an important role in drug discovery. HM analysis aims at predicting a 3D model from a biological sequence in order to discover new drugs. There are several problems in executing an HM analysis in large-scale, such as multiple software to be evaluated, the management of the parallel execution, and results analysis, e.g. browsing manually all results to find which structure was derived from which program with good quality. Scientific Workflow Management System (SWfMS) with parallelism and provenance support can aid the large-scale HM executions by addressing the result analysis. However, before submitting the HM workflow for execution, it has to be specified along with its several alternatives (also called variants), as considered in this paper. Managing HM workflow variations is a complex task to be accomplished even with the help of a SWfMS. In this paper, we propose SciSamma (Structural Approach and Molecular Modeling Analyses), an abstract representation of HM workflows inspired in the concept of software product lines (SPL). SciSamma models HM workflow variants to execute with parallel processing in the cloud using SciCumulus SWfMS. We evaluated SciSamma with two common variants using 100 protease enzymes of protozoan genomes. Both variations presented scalability with performance improvements (dropping from 8 h to 27 min using 32 Amazon's large virtual machines). While evaluating the two workflow variants, through provenance queries, they present the same quality in biological results, but the difference in execution time between them was around 40 {\%}.},
  author          = {Oca{\~{n}}a, Kary A.C.S. and de Oliveira, Daniel and Silva, V{\'{i}}tor and Benza, Silvia and Mattoso, Marta},
  booktitle       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  doi             = {10.1007/978-3-319-22885-3_29},
  isbn            = {9783319228846},
  issn            = {16113349},
  keywords        = {Cloud,Homology modeling,Provenance data,Workflow},
  mendeley-groups = {MestradoEngnr/Disserta{\c{c}}{\~{a}}o/Papers Marta},
  pages           = {336--350},
  title           = {{Exploiting the parallel execution of homology workflow alternatives in HPC compute clouds}},
  url             = {http://link.springer.com/10.1007/978-3-319-22885-3{\_}29},
  volume          = {8954},
  year            = {2015}
}
@article{Virtanen2020,
  abstract        = {SciPy is an open-source scientific computing library for the Python programming language. Since its initial release in 2001, SciPy has become a de facto standard for leveraging scientific algorithms in Python, with over 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories and millions of downloads per year. In this work, we provide an overview of the capabilities and development practices of SciPy 1.0 and highlight some recent technical developments.},
  archiveprefix   = {arXiv},
  arxivid         = {1907.10121},
  author          = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and van der Walt, St{\'{e}}fan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R.J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C. J. and Polat, İlhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Ant{\^{o}}nio H. and Pedregosa, Fabian and van Mulbregt, Paul and Vijaykumar, Aditya and Bardelli, Alessandro Pietro and Rothberg, Alex and Hilboll, Andreas and Kloeckner, Andreas and Scopatz, Anthony and Lee, Antony and Rokem, Ariel and Woods, C. Nathan and Fulton, Chad and Masson, Charles and H{\"{a}}ggstr{\"{o}}m, Christian and Fitzgerald, Clark and Nicholson, David A. and Hagen, David R. and Pasechnik, Dmitrii V. and Olivetti, Emanuele and Martin, Eric and Wieser, Eric and Silva, Fabrice and Lenders, Felix and Wilhelm, Florian and Young, G. and Price, Gavin A. and Ingold, Gert Ludwig and Allen, Gregory E. and Lee, Gregory R. and Audren, Herv{\'{e}} and Probst, Irvin and Dietrich, J{\"{o}}rg P. and Silterra, Jacob and Webber, James T. and Slavi{\v{c}}, Janko and Nothman, Joel and Buchner, Johannes and Kulick, Johannes and Sch{\"{o}}nberger, Johannes L. and {de Miranda Cardoso}, Jos{\'{e}} Vin{\'{i}}cius and Reimer, Joscha and Harrington, Joseph and Rodr{\'{i}}guez, Juan Luis Cano and Nunez-Iglesias, Juan and Kuczynski, Justin and Tritz, Kevin and Thoma, Martin and Newville, Matthew and K{\"{u}}mmerer, Matthias and Bolingbroke, Maximilian and Tartre, Michael and Pak, Mikhail and Smith, Nathaniel J. and Nowaczyk, Nikolai and Shebanov, Nikolay and Pavlyk, Oleksandr and Brodtkorb, Per A. and Lee, Perry and McGibbon, Robert T. and Feldbauer, Roman and Lewis, Sam and Tygier, Sam and Sievert, Scott and Vigna, Sebastiano and Peterson, Stefan and More, Surhud and Pudlik, Tadeusz and Oshima, Takuya and Pingel, Thomas J. and Robitaille, Thomas P. and Spura, Thomas and Jones, Thouis R. and Cera, Tim and Leslie, Tim and Zito, Tiziano and Krauss, Tom and Upadhyay, Utkarsh and Halchenko, Yaroslav O. and V{\'{a}}zquez-Baeza, Yoshiki},
  doi             = {10.1038/s41592-019-0686-2},
  eprint          = {1907.10121},
  issn            = {15487105},
  journal         = {Nature Methods},
  mendeley-groups = {MestradoMicro/FumitoVibrios/Methods},
  pmid            = {32015543},
  title           = {{SciPy 1.0: fundamental algorithms for scientific computing in Python}},
  year            = {2020}
}
@article{Hunter2007,
  abstract = {Matplotlib is a 2D graphics package used for Python for application development, interactive scripting, and publication-quality image generation across user interfaces and operating systems.},
  author   = {Hunter, John D.},
  doi      = {10.1109/MCSE.2007.55},
  issn     = {1521-9615},
  journal  = {Computing in Science {\&} Engineering},
  number   = {3},
  pages    = {90--95},
  title    = {{Matplotlib: A 2D Graphics Environment}},
  url      = {http://ieeexplore.ieee.org/document/4160265/},
  volume   = {9},
  year     = {2007}
}
@misc{Harris2020,
  abstract      = {Array programming provides a powerful, compact and expressive syntax for accessing, manipulating and operating on data in vectors, matrices and higher-dimensional arrays. NumPy is the primary array programming library for the Python language. It has an essential role in research analysis pipelines in fields as diverse as physics, chemistry, astronomy, geoscience, biology, psychology, materials science, engineering, finance and economics. For example, in astronomy, NumPy was an important part of the software stack used in the discovery of gravitational waves1 and in the first imaging of a black hole2. Here we review how a few fundamental array concepts lead to a simple and powerful programming paradigm for organizing, exploring and analysing scientific data. NumPy is the foundation upon which the scientific Python ecosystem is constructed. It is so pervasive that several projects, targeting audiences with specialized needs, have developed their own NumPy-like interfaces and array objects. Owing to its central position in the ecosystem, NumPy increasingly acts as an interoperability layer between such array computation libraries and, together with its application programming interface (API), provides a flexible framework to support the next decade of scientific and industrial analysis.},
  archiveprefix = {arXiv},
  arxivid       = {2006.10256},
  author        = {Harris, Charles R. and Millman, K. Jarrod and van der Walt, St{\'{e}}fan J. and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J. and Kern, Robert and Picus, Matti and Hoyer, Stephan and van Kerkwijk, Marten H. and Brett, Matthew and Haldane, Allan and del R{\'{i}}o, Jaime Fern{\'{a}}ndez and Wiebe, Mark and Peterson, Pearu and G{\'{e}}rard-Marchant, Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant, Travis E.},
  booktitle     = {Nature},
  doi           = {10.1038/s41586-020-2649-2},
  eprint        = {2006.10256},
  issn          = {14764687},
  number        = {7825},
  pages         = {357--362},
  pmid          = {32939066},
  title         = {{Array programming with NumPy}},
  volume        = {585},
  year          = {2020}
}
@article{DePaula2013,
  abstract  = {In this work, we used the PROV-DM model to manage data provenance in workflows of genome projects. This provenance model allows the storage of details of one workflow execution, e.g., raw and produced data and computational tools, their versions and parameters. Using this model, biologists can access details of one particular execution of a workflow, compare results produced by different executions, and plan new experiments more efficiently. In addition to this, a provenance simulator was created, which facilitates the inclusion of provenance data of one genome project workflow execution. Finally, we discuss one case study, which aims to identify genes involved in specific metabolic pathways of Bacillus cereus, as well as to compare this isolate with other phylogenetic related bacteria from the Bacillus group. B. cereus is an extremophilic bacteria, collected in warm water in the Midwestern Region of Brazil, its DNA samples having been sequenced with an NGS machine.},
  author    = {de Paula, Renato and Holanda, Maristela and Gomes, Luciana S.A. and Lifschitz, Sergio and Walter, Maria Emilia M.T.},
  doi       = {10.1186/1471-2105-14-S11-S6},
  issn      = {14712105},
  journal   = {BMC bioinformatics},
  number    = {Suppl 11},
  pages     = {S6},
  publisher = {BioMed Central},
  title     = {{Provenance in bioinformatics workflows.}},
  volume    = {14 Suppl 1},
  year      = {2013}
}
@article{DiTommaso2017,
  abstract = {In spite of over two decades of intense research, illumination and pose invariance remain prohibitively challenging aspects of face recognition for most practical applications. The objective of this work is to recognize faces using video sequences both for training and recognition input, in a realistic, unconstrained setup in which lighting, pose and user motion pattern have a wide variability and face images are of low resolution. The central contribution is an illumination invariant, which we show to be suitable for recognition from video of loosely constrained head motion. In particular there are three contributions: (i) we show how a photometric model of image formation can be combined with a statistical model of generic face appearance variation to exploit the proposed invariant and generalize in the presence of extreme illumination changes; (ii) we introduce a video sequence re-illumination algorithm to achieve fine alignment of two video sequences; and (iii) we use the smoothness of geodesically local appearance manifold structure and a robust same-identity likelihood to achieve robustness to unseen head poses. We describe a fully automatic recognition system based on the proposed method and an extensive evaluation on 323 individuals and 1474 video sequences with extreme illumination, pose and head motion variation. Our system consistently achieved a nearly perfect recognition rate (over 99.7{\%} on all four databases). ?? 2012 Elsevier Ltd All rights reserved.},
  author   = {{Di Tommaso}, Paolo and Chatzou, Maria and Floden, Evan W. and Barja, Pablo Prieto and Palumbo, Emilio and Notredame, Cedric},
  doi      = {10.1038/nbt.3820},
  issn     = {15461696},
  journal  = {Nature Biotechnology},
  number   = {4},
  pages    = {316--319},
  title    = {{Nextflow enables reproducible computational workflows}},
  volume   = {35},
  year     = {2017}
}

@misc{Dong2020,
  author    = {Dong, Trung},
  journal   = {GitHub repository},
  publisher = {GitHub},
  title     = {{PROV: A Python library for W3C Provenance Data Model}},
  year      = {2020}
}
@article{Edgar2004,
  abstract = {We describe MUSCLE, a new computer program for creating multiple alignments of protein sequences. Elements of the algorithm include fast distance estimation using kmer counting, progressive alignment using a new profile function we call the log-expectation score, and refinement using tree-dependent restricted partitioning. The speed and accuracy of MUSCLE are compared with T-Coffee, MAFFT and CLUSTALW on four test sets of reference alignments: BAliBASE, SABmark, SMART and a new benchmark, PREFAB. MUSCLE achieves the highest, or joint highest, rank in accuracy on each of these sets. Without refinement, MUSCLE achieves average accuracy statistically indistinguishable from T-Coffee and MAFFT, and is the fastest of the tested methods for large numbers of sequences, aligning 5000 sequences of average length 350 in 7 min on a current desktop computer. The MUSCLE program, source code and PREFAB test data are freely available at http://www.drive5.com/muscle. {\textcopyright} Oxford University Press 20004; all rights reserved.},
  author   = {Edgar, Robert C.},
  doi      = {10.1093/nar/gkh340},
  issn     = {03051048},
  journal  = {Nucleic Acids Research},
  number   = {5},
  pages    = {1792--1797},
  pmid     = {15034147},
  title    = {{MUSCLE: Multiple sequence alignment with high accuracy and high throughput}},
  volume   = {32},
  year     = {2004}
}
@misc{Vivian2017,
  author    = {Vivian, John and Rao, Arjun Arkal and Nothaft, Frank Austin and Ketchum, Christopher and Armstrong, Joel and Novak, Adam and Pfeil, Jacob and Narkizian, Jake and Deran, Alden D. and Musselman-Brown, Audrey and Schmidt, Hannes and Amstutz, Peter and Craft, Brian and Goldman, Mary and Rosenbloom, Kate and Cline, Melissa and O'Connor, Brian and Hanna, Megan and Birger, Chet and Kent, W. James and Patterson, David A. and Joseph, Anthony D. and Zhu, Jingchun and Zaranek, Sasha and Getz, Gad and Haussler, David and Paten, Benedict},
  booktitle = {Nature Biotechnology},
  doi       = {10.1038/nbt.3772},
  issn      = {15461696},
  number    = {4},
  pages     = {314--316},
  pmid      = {28398314},
  title     = {{Toil enables reproducible, open source, big biomedical data analyses}},
  volume    = {35},
  year      = {2017}
}
@article{Groth2013,
  title     = {Provenance: an introduction to PROV},
  author    = {Moreau, Luc and Groth, Paul},
  journal   = {Synthesis lectures on the semantic web: theory and technology},
  volume    = {3},
  number    = {4},
  pages     = {1--129},
  year      = {2013},
  publisher = {Morgan \& Claypool Publishers},
  doi       = {10.2200/S00528ED1V01Y201308WBE007}
}
@article{Hyatt2010,
  abstract = {BACKGROUND: The quality of automated gene prediction in microbial organisms has improved steadily over the past decade, but there is still room for improvement. Increasing the number of correct identifications, both of genes and of the translation initiation sites for each gene, and reducing the overall number of false positives, are all desirable goals.$\backslash$n$\backslash$nRESULTS: With our years of experience in manually curating genomes for the Joint Genome Institute, we developed a new gene prediction algorithm called Prodigal (PROkaryotic DYnamic programming Gene-finding ALgorithm). With Prodigal, we focused specifically on the three goals of improved gene structure prediction, improved translation initiation site recognition, and reduced false positives. We compared the results of Prodigal to existing gene-finding methods to demonstrate that it met each of these objectives.$\backslash$n$\backslash$nCONCLUSION: We built a fast, lightweight, open source gene prediction program called Prodigal http://compbio.ornl.gov/prodigal/. Prodigal achieved good results compared to existing methods, and we believe it will be a valuable asset to automated microbial annotation pipelines.},
  author   = {Hyatt, Doug and Chen, Gwo-Liang and Locascio, Philip F and Land, Miriam L and Larimer, Frank W and Hauser, Loren J},
  doi      = {10.1186/1471-2105-11-119},
  isbn     = {1471-2105 (Electronic)$\backslash$r1471-2105 (Linking)},
  issn     = {1471-2105},
  journal  = {BMC bioinformatics},
  keywords = {Algorithms,Bacterial,Databases,Genetic,Genome,Peptide Chain Initiation,Prokaryotic Cells,Software,Translational,Translational: genetics},
  pages    = {119},
  pmid     = {20211023},
  title    = {{Prodigal: prokaryotic gene recognition and translation initiation site identification.}},
  url      = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2848648{\&}tool=pmcentrez{\&}rendertype=abstract},
  volume   = {11},
  year     = {2010}
}
@article{Koster2012,
  abstract = {Snakemake is a workflow engine that provides a readable Python-based workflow definition language and a powerful execution environment that scales from single-core workstations to compute clusters without modifying the workflow. It is the first system to support the use of automatically inferred multiple named wildcards (or variables) in input and output filenames. {\textcopyright} The Author 2012. Published by Oxford University Press. All rights reserved.},
  author   = {K{\"{o}}ster, Johannes and Rahmann, Sven},
  doi      = {10.1093/bioinformatics/bts480},
  issn     = {14602059},
  journal  = {Bioinformatics},
  number   = {19},
  pages    = {2520--2522},
  pmid     = {29788404},
  title    = {{Snakemake-a scalable bioinformatics workflow engine}},
  volume   = {28},
  year     = {2012}
}
@article{Lakin2017,
  abstract  = {{\textcopyright} The Author(s) 2016. Antimicrobial resistance has become an imminent concern for public health. As methods for detection and characterization of antimicrobial resistance move from targeted culture and polymerase chain reaction to high throughput metagenomics, appropriate resources for the analysis of large-scale data are required. Currently, antimicrobial resistance databases are tailored to smaller-scale, functional profiling of genes using highly descriptive annotations. Such characteristics do not facilitate the analysis of large-scale, ecological sequence datasets such as those produced with the use of metagenomics for surveillance. In order to overcome these limitations, we present MEGARes (https://megares.meglab.org), a hand-curated antimicrobial resistance database and annotation structure that provides a foundation for the development of high throughput acyclical classifiers and hierarchical statistical analysis of big data. MEGARes can be browsed as a stand-alone resource through the website or can be easily integrated into sequence analysis pipelines through download. Also via the website, we provide documentation for AmrPlusPlus, a user-friendly Galaxy pipeline for the analysis of high throughput sequencing data that is pre-packaged for use with the MEGARes database.},
  author    = {Lakin, Steven M. and Dean, Chris and Noyes, Noelle R. and Dettenwanger, Adam and Ross, Anne Spencer and Doster, Enrique and Rovira, Pablo and Abdo, Zaid and Jones, Kenneth L. and Ruiz, Jaime and Belk, Keith E. and Morley, Paul S. and Boucher, Christina},
  doi       = {10.1093/nar/gkw1009},
  issn      = {13624962},
  journal   = {Nucleic Acids Research},
  month     = {jan},
  number    = {D1},
  pages     = {D574--D580},
  publisher = {Oxford University Press},
  title     = {{MEGARes: An antimicrobial resistance database for high throughput sequencing}},
  volume    = {45},
  year      = {2017}
}
@article{Markowetz2017,
  author  = {Markowetz, Florian},
  doi     = {10.1371/journal.pbio.2002050},
  issn    = {15457885},
  journal = {PLoS Biology},
  number  = {3},
  pages   = {4--7},
  pmid    = {28278152},
  title   = {{All biology is computational biology}},
  volume  = {15},
  year    = {2017}
}
@article{Mattoso2010,
  author  = {Mattoso, Marta and Werner, Claudia and Travassos, Guilherme Horta and Braganholo, Vanessa and Ogasawara, Eduardo and Oliveira, Daniel De and Cruz, Sergio Manuel Serra Da and Martinho, Wallace and Murta, Leonardo},
  doi     = {10.1504/ijbpim.2010.033176},
  issn    = {1741-8763},
  journal = {International Journal of Business Process Integration and Management},
  number  = {1},
  pages   = {79},
  title   = {{Towards supporting the life cycle of large scale scientific experiments}},
  volume  = {5},
  year    = {2010}
}
@article{Kanwal2017,
  abstract  = {Background: Computational bioinformatics workflows are extensively used to analyse genomics data, with different approaches available to support implementation and execution of these workflows. Reproducibility is one of the core principles for any scientific workflow and remains a challenge, which is not fully addressed. This is due to incomplete understanding of reproducibility requirements and assumptions of workflow definition approaches. Provenance information should be tracked and used to capture all these requirements supporting reusability of existing workflows. Results: We have implemented a complex but widely deployed bioinformatics workflow using three representative approaches to workflow definition and execution. Through implementation, we identified assumptions implicit in these approaches that ultimately produce insufficient documentation of workflow requirements resulting in failed execution of the workflow. This study proposes a set of recommendations that aims to mitigate these assumptions and guides the scientific community to accomplish reproducible science, hence addressing reproducibility crisis. Conclusions: Reproducing, adapting or even repeating a bioinformatics workflow in any environment requires substantial technical knowledge of the workflow execution environment, resolving analysis assumptions and rigorous compliance with reproducibility requirements. Towards these goals, we propose conclusive recommendations that along with an explicit declaration of workflow specification would result in enhanced reproducibility of computational genomic analyses.},
  author    = {Kanwal, Sehrish and Khan, Farah Zaib and Lonie, Andrew and Sinnott, Richard O.},
  doi       = {10.1186/s12859-017-1747-0},
  issn      = {14712105},
  journal   = {BMC Bioinformatics},
  keywords  = {Common Workflow Language (CWL),Cpipe,Galaxy,Provenance,Reproducibility,Workflow},
  month     = {jul},
  number    = {1},
  pages     = {337},
  publisher = {BioMed Central Ltd.},
  title     = {{Investigating reproducibility and tracking provenance - A genomic workflow case study}},
  url       = {http://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-017-1747-0},
  volume    = {18},
  year      = {2017}
}
@article{Katoh2005,
  abstract = {The accuracy of multiple sequence alignment proram MAFFT has been improved. The new version (5.3) of MAFFT offers new iterative refinement options, H-INS-i, F-INS-i and G-INS-i, in which pairwise alignment information are incorporated into objective function. These new options of MAFFT showed higher accuracy than currently available methods including TCoffee version 2 and CLUSTAL W in benchmark tests consisting of alignments of {\textgreater}50 sequences. Like the previously available options, the new options of MAFFT can handle hundreds of sequences on a standard desktop computer. We also examined the effect of the number of homologues included in an alignment. For a multiple alignment consisting of ∼8 sequences with low similarity, the accuracy was improved (2-10 percentage points) when the sequences were aligned together with dozens of their close homologues (E-value {\textless} 10-5-10-20) collected from a database. Such improvement was generally observed for most methods, but remarkably large for the new options of MAFFT proposed here. Thus, we made a Ruby script, mafftE.rb, which aligns the input sequences together with their close homologues collected from SwissProt using NCBI-BLAST. {\textcopyright} Oxford University Press 2005; all rights reserved.},
  author   = {Katoh, Kazutaka and Kuma, Kei Ichi and Toh, Hiroyuki and Miyata, Takashi},
  doi      = {10.1093/nar/gki198},
  issn     = {03051048},
  journal  = {Nucleic Acids Research},
  number   = {2},
  pages    = {511--518},
  pmid     = {15661851},
  title    = {{MAFFT version 5: Improvement in accuracy of multiple sequence alignment}},
  volume   = {33},
  year     = {2005}
}
@article{Khan2019,
  abstract = {Background: The automation of data analysis in the form of scientific workflows has become a widely adopted practice in many fields of research. Computationally driven data-intensive experiments using workflows enable automation, scaling, adaptation, and provenance support. However, there are still several challenges associated with the effective sharing, publication, and reproducibility of such workflows due to the incomplete capture of provenance and lack of interoperability between different technical (software) platforms. Results: Based on best-practice recommendations identified from the literature on workflow design, sharing, and publishing, we define a hierarchical provenance framework to achieve uniformity in provenance and support comprehensive and fully re-executable workflows equipped with domain-specific information. To realize this framework, we present CWLProv, a standard-based format to represent any workflow-based computational analysis to produce workflow output artefacts that satisfy the various levels of provenance. We use open source community-driven standards, interoperable workflow definitions in Common Workflow Language (CWL), structured provenance representation using the W3C PROV model, and resource aggregation and sharing as workflow-centric research objects generated along with the final outputs of a given workflow enactment. We demonstrate the utility of this approach through a practical implementation of CWLProv and evaluation using real-life genomic workflows developed by independent groups. Conclusions: The underlying principles of the standards utilized by CWLProv enable semantically rich and executable research objects that capture computational workflows with retrospective provenance such that any platform supporting CWL will be able to understand the analysis, reuse the methods for partial reruns, or reproduce the analysis to validate the published findings.},
  author   = {Khan, Farah Zaib and Soiland-Reyes, Stian and Sinnott, Richard O. and Lonie, Andrew and Goble, Carole and Crusoe, Michael R.},
  doi      = {10.1093/gigascience/giz095},
  issn     = {2047217X},
  journal  = {GigaScience},
  keywords = {BagIt,CWL,Common Workflow Language,RO,Research Object,containers,interoperability,provenance,scientific workflows},
  number   = {11},
  pmid     = {31675414},
  title    = {{Sharing interoperable workflow provenance: A review of best practices and their practical application in CWLProv}},
  volume   = {8},
  year     = {2019}
}

@article{Pasquier2017,
  author  = {Pasquier, Thomas and Lau, Matthew K. and Trisovic, Ana and Boose, Emery R. and Couturier, Ben and Crosas, Merc{\`{e}} and Ellison, Aaron M. and Gibson, Valerie and Jones, Chris R. and Seltzer, Margo},
  doi     = {10.1038/sdata.2017.114},
  issn    = {20524463},
  journal = {Scientific Data},
  pages   = {1--5},
  title   = {{If these data could talk}},
  volume  = {4},
  year    = {2017}
}
@inproceedings{ragan2014jupyter,
  author    = {Ragan-Kelley, Min and Perez, F and Granger, B and Kluyver, T and Ivanov, P and Frederic, J and Bussonnier, M},
  booktitle = {AGU Fall Meeting Abstracts},
  title     = {{The Jupyter/IPython architecture: a unified view of computational research, from interactive exploration to communication and publication.}},
  year      = {2014}
}

@article{Silva2018,
  abstract = {We present DfAnalyzer, a tool that enables monitoring, debugging, steering, and analysis of dataflows while being generated by scientific applications. It works by capturing strategic domain data, registering provenance and execution data to enable queries at runtime. DfAnalyzer provides lightweight dataflow monitoring components to be invoked by high performance applications. It can be plugged in scientific code scripts, or Spark applications, in the same way users already plug visualization library components. During this demo, we will show how DfAnalyzer captures the dataflow, provenance, as well as how it provides runtime data analyses of applications. We will also encourage attendees to use DfAnalyzer for their own applications.},
  author   = {Silva, V{\'{i}}tor and de Oliveira, Daniel and Valduriez, Patrick and Mattoso, Marta},
  doi      = {10.14778/3229863.3236265},
  number   = {12},
  pages    = {2082--2085},
  title    = {{DfAnalyzer: Runtime Dataflow Analysis of Scientific Applications using Provenance}},
  url      = {https://doi.org/10.14778/3229863.3236265},
  volume   = {11},
  year     = {2018}
}
@article{Stevens2007,
  author   = {Stevens, Robert and Zhao, Jun and Goble, Carole},
  doi      = {10.1093/bib/bbm015},
  issn     = {14675463},
  journal  = {Briefings in Bioinformatics},
  keywords = {Data derivation,In Silico experiments,Provenance,Validation and verification of results,Workflow},
  number   = {3},
  pages    = {183--194},
  title    = {{Using provenance to manage knowledge of In Silico experiments}},
  volume   = {8},
  year     = {2007}
}
@article{wilson2017good,
  author    = {Wilson, Greg and Bryan, Jennifer and Cranston, Karen and Kitzes, Justin and Nederbragt, Lex and Teal, Tracy K},
  journal   = {PLOS Computational Biology},
  number    = {6},
  doi       = {10.1371/journal.pcbi.1005510},
  pages     = {e1005510},
  publisher = {Public Library of Science},
  title     = {{Good enough practices in scientific computing}},
  volume    = {13},
  year      = {2017}
}
